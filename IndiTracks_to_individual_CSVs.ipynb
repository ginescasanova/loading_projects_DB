{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Import the necessary libraries \n",
    "from office365.runtime.auth.authentication_context import AuthenticationContext\n",
    "from office365.sharepoint.client_context import ClientContext\n",
    "from office365.sharepoint.files.file import File\n",
    "from openpyxl import load_workbook\n",
    "from urllib import parse \n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#2a. Set up credentials:\n",
    "\n",
    "# Get environmental variables:\n",
    "USERNAME = os.environ.get('O365_CCR_USERNAME')\n",
    "PASSWORD = os.environ.get('O365_CCR_PASSWORD')\n",
    "ROOT=os.environ.get('OneDrive') #the resulting CSVs are loaded in OneDrive before they are uploaded to SharePoint\n",
    "\n",
    "\n",
    "#read json config file:\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "    config = config[\"share_point\"]\n",
    "\n",
    "#extract variables from the json file to use them in code:\n",
    "url_hprs=config[\"site\"]\n",
    "selected_folder=config[\"inditracks_folder\"] #the intermediary OneDrive folder where the resulting CSVs are stored before they are uploaded into SharePoint\n",
    "\n",
    "file_destination= ROOT + selected_folder\n",
    "\n",
    "#2b. creates a dataframe with the sites and lists to be iterated:\n",
    "with open(\"General_ShP_sites.json\") as sites_file:\n",
    "    shp_sites = json.load(sites_file)\n",
    "\n",
    "df_shp_sites=pd.DataFrame(shp_sites)   \n",
    "\n",
    "current_inditracks=pd.read_csv(ROOT + \"\\\\Python\\\\Current_IndiTracks.csv\", index_col=0)\n",
    "current_inditracks=current_inditracks.reset_index()\n",
    "timestamp=datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Define function to connect to the SharePoint site:\n",
    "def autenticate_in_sharepoint(shp_url, USERNAME, PASSWORD):    \n",
    "    ctx_auth = AuthenticationContext(shp_url)\n",
    "    if ctx_auth.acquire_token_for_user(USERNAME, PASSWORD):\n",
    "        ctx = ClientContext(shp_url, ctx_auth)\n",
    "        web = ctx.web\n",
    "        ctx.load(web)\n",
    "        ctx.execute_query()\n",
    "        print (\"\")\n",
    "        print (\"\")\n",
    "        print (\"\")\n",
    "        print (\"\")\n",
    "\n",
    "        print(\"The Automat has connected to: {0}\".format(web.properties['Title']))\n",
    "\n",
    "    else:\n",
    "        print (\"\")\n",
    "        print (ctx_auth.get_last_error())\n",
    "        \n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Define function to process all tables and produce project dataframes:\n",
    "\n",
    "def download_inditrack (ctx, project_code, relative_url):\n",
    "    \n",
    "    print (\"\")\n",
    "    print(\"Project \" + project_code + \" is being processed...\")\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #4a. Download the file:\n",
    "\n",
    "        response = File.open_binary(ctx, relative_url)\n",
    "\n",
    "        #save data to BytesIO stream\n",
    "        bytes_file_obj = io.BytesIO()\n",
    "        bytes_file_obj.write(response.content)\n",
    "        bytes_file_obj.seek(0) #set file object to start\n",
    "\n",
    "\n",
    "        #4b. Importing Excel´s table objects inside of the workbook with openpyxl:\n",
    "\n",
    "        wb = load_workbook(bytes_file_obj, data_only=True)\n",
    "\n",
    "        print (\"\")\n",
    "        print(\"These are the available sheets in \" + project_code + \"´s workbook:\")\n",
    "        print(wb.sheetnames)\n",
    "\n",
    "        ws1 = wb[\"M&E Plan\"] #explore one specific sheet\n",
    "        #print(ws1.tables.items()) # list the tables inside of the selected sheet\n",
    "        ws2 = wb[\"Events\"] #explore one specific sheet\n",
    "        #print(ws2.tables.items()) # list the tables inside of the selected sheet\n",
    "        ws3 = wb[\"Milestones\"] #explore one specific sheet\n",
    "        #print(ws3.tables.items()) # list the tables inside of the selected sheet\n",
    "\n",
    "        sheets_list=[ws1,ws2,ws3]\n",
    "\n",
    "        #4c. Creating dataframes out of each excel table:\n",
    "        mapping = {}\n",
    "        for sheet in sheets_list:\n",
    "            for entry, data_boundary in sheet.tables.items():\n",
    "                #parse the data within the ref boundary\n",
    "                data = sheet[data_boundary]\n",
    "                \n",
    "                #extract the data \n",
    "                #the inner list comprehension gets the values for each cell in the table\n",
    "                content = [[cell.value for cell in ent] \n",
    "                           for ent in data\n",
    "                      ]\n",
    "\n",
    "                header = content[0]\n",
    "\n",
    "                #the contents ... excluding the header\n",
    "                rest = content[1:]\n",
    "\n",
    "                #create dataframe with the column names\n",
    "                #and pair table name with dataframe\n",
    "                df = pd.DataFrame(rest, columns = header)\n",
    "                mapping[entry] = df\n",
    "\n",
    "        Outcomes, Activities, Indicators, Milestones=mapping.values()\n",
    "\n",
    "        return Outcomes,Activities, Indicators, Milestones \n",
    "\n",
    "    except: \n",
    "        print(\"It has not been possible to load \" + project_code + \"'s workbook         #WARNING: failure on load in \" + project_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Define function that creates the DIM table for outcomes of type \"progress\":\n",
    "def dim_outc_progress (Outcomes, project_code, timestamp):\n",
    "    try:\n",
    "        DIM_outcome_progress = Outcomes\n",
    "        DIM_outcome_progress [\"Indicator level\"]=\"Outcomes\"\n",
    "        DIM_outcome_progress [\"Project Code\"]=project_code\n",
    "        DIM_outcome_progress [\"Version\"]= timestamp\n",
    "        DIM_outcome_progress [\"indicator_key\"]=DIM_outcome_progress [\"Project Code\"]+\"_\"+DIM_outcome_progress [\"Indicator ID\"]\n",
    "\n",
    "        DIM_outcome_progress = Outcomes[Outcomes[\"Indicator Type\"]==\"Progress\"][[\"indicator_key\",\"Project Code\",\"Indicator level\", \"Outcome ID\", \"Outcomes (O)\",\n",
    "                                            \"Indicator ID\",\"Definition of the indicator\", \"Overall target\", \"Indicator Total Weight in LogFrame)\",\n",
    "                                            \"Specification of the indicator (if needed after reading column D)\",\"Source of verification\",\n",
    "                                            \"Predicted number of events within this indicator\", \"Indicator Type\", \"Version\"]]\n",
    "\n",
    "        DIM_outcome_progress.rename(columns = {'Indicator Total Weight in LogFrame)':'Indicator Total Weight in LogFrame'}, inplace=True)\n",
    "\n",
    "\n",
    "        DIM_outcome_progress_indexed=DIM_outcome_progress.set_index(\"indicator_key\")\n",
    "        csv_name=project_code+\"_DIM_outcome_progress.csv\"\n",
    "        DIM_outcome_progress_indexed.to_csv(file_destination+csv_name)\n",
    "        upload_file_to_sharepoint(file_destination,csv_name)\n",
    "\n",
    "        \n",
    "        print (\"\")\n",
    "        print (\"Setting outcome dataframes for \" + project_code)\n",
    "        print (\"\")\n",
    "        print (\"    'DIM_outcome_progress' created...\")\n",
    "        return DIM_outcome_progress\n",
    "    except:       \n",
    "        print (\"    'DIM_outcome_progress' creation failed...         #WARNING: failure on load in \" + project_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Define function that creates the dataframe \"outc_expected_time\" \n",
    "#by merging DIM Outcomes and Indicators, filtering, and implementing some adjustments:\n",
    "\n",
    "def outc_exp_time(DIM_outcome_progress, Indicators):\n",
    "    try:\n",
    "        outc_expected_time=DIM_outcome_progress.merge(Indicators, on=\"Indicator ID\")\n",
    "        outc_expected_time[\"event_key\"]=outc_expected_time [\"Project Code\"]+\"_\"+outc_expected_time [\"Event ID\"]\n",
    "\n",
    "\n",
    "        outc_expected_time= outc_expected_time[[\"event_key\",\"Project Code\",\"Activity_ID/Outcome_ID\",\"Indicator ID\",\"Description\",\"Event ID\",\n",
    "                                                    \"Planned date of the event (what is expected)\",\n",
    "                                                    \"Planned value of the indicator (what is expected)\", \"Indicator level\", \n",
    "                                                    \"Overall target\",\"Indicator Total Weight in LogFrame\",\"Indicator Type\",\n",
    "                                                    \"Version\"]]\n",
    "\n",
    "        outc_expected_time=outc_expected_time[(outc_expected_time[\"Indicator level\"]==\"Outcomes\") & (outc_expected_time[\"Indicator Type\"]==\"Progress\")]\n",
    "\n",
    "        #Creates calculated column \"Expected_progress_Outc\"\n",
    "        outc_expected_time[\"Expected_progress_Outc\"]=outc_expected_time[\"Planned value of the indicator (what is expected)\"]/outc_expected_time[\"Overall target\"]\n",
    "        #Creates calculated column \"Expected_Progress_Outc_Weighted\"\n",
    "        outc_expected_time[\"Expected_Progress_Outc_Weighted\"]=outc_expected_time[\"Expected_progress_Outc\"]*outc_expected_time[\"Indicator Total Weight in LogFrame\"]\n",
    "\n",
    "        print (\"    'outc_expected_time' ckecks have started...\")\n",
    "        \n",
    "        #Format universal time into \"dd/mm/yyyy\"\n",
    "        outc_expected_time=debug_date_column(outc_expected_time) #function n.15\n",
    "\n",
    "        #Check for data consistency: indicator´s overal target in M+E Plan must match the sum of planned values of all progress events:\n",
    "        target_vs_values(DIM_outcome_progress, outc_expected_time) #function n.16\n",
    "\n",
    "        outc_expected_time_indexed=outc_expected_time.set_index(\"event_key\")\n",
    "        csv_name=project_code+\"_outc_expected_time.csv\"\n",
    "        outc_expected_time_indexed.to_csv(file_destination+csv_name)\n",
    "        upload_file_to_sharepoint(file_destination,csv_name)\n",
    "\n",
    "\n",
    "        print (\"    'outc_expected_time' created...\")\n",
    "\n",
    "        return outc_expected_time\n",
    "    \n",
    "    except:       \n",
    "        print (\"    'outc_expected_time' creation failed...         #WARNING: failure on load in \" + project_code)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Define function that creates the dataframe \"outc_reported_time\" \n",
    "#by merging DIM Outcomes and Indicators, filtering, and implementing some adjustments:\n",
    "\n",
    "def outc_rep_time(DIM_outcome_progress, Indicators):\n",
    "    try:\n",
    "        outc_reported_time=DIM_outcome_progress.merge(Indicators, on=\"Indicator ID\")\n",
    "        outc_reported_time[\"event_key\"]=outc_reported_time [\"Project Code\"]+\"_\"+outc_reported_time [\"Event ID\"]\n",
    "\n",
    "        outc_reported_time= outc_reported_time[[\"event_key\",\"Project Code\",\"Activity_ID/Outcome_ID\",\"Indicator ID\",\"Description\",\"Event ID\",\n",
    "                                                    \"Actual date of the event (when it really happened)\",\n",
    "                                                    \"Value of the indicator (real value after event)\", \"Indicator level\", \n",
    "                                                    \"Overall target\",\"Indicator Total Weight in LogFrame\",\"Indicator Type\",\n",
    "                                                    \"Version\"]]\n",
    "\n",
    "        outc_reported_time=outc_reported_time[(outc_reported_time[\"Indicator level\"]==\"Outcomes\") & (outc_reported_time[\"Indicator Type\"]==\"Progress\")]\n",
    "\n",
    "        #Drop non-reported values:\n",
    "        outc_reported_time=outc_reported_time.dropna()\n",
    "\n",
    "        \n",
    "        #Create calculated column \"Real Progress\"\n",
    "        outc_reported_time[\"Real Progress\"]=outc_reported_time[\"Value of the indicator (real value after event)\"]/outc_reported_time[\"Overall target\"]\n",
    "\n",
    "        print (\"    'outc_reported_time' ckecks have started...\")\n",
    "\n",
    "        #Format universal time into \"dd/mm/yyyy\"\n",
    "        outc_reported_time=debug_date_column(outc_reported_time) #function n.15\n",
    "\n",
    "        outc_reported_time_indexed=outc_reported_time.set_index(\"event_key\")\n",
    "        csv_name=project_code+\"_outc_reported_time.csv\"\n",
    "        outc_reported_time_indexed.to_csv(file_destination+csv_name)\n",
    "        upload_file_to_sharepoint(file_destination,csv_name)\n",
    "\n",
    "        \n",
    "        print (\"    'outc_reported_time' created...\")\n",
    "\n",
    "        return outc_reported_time\n",
    "    \n",
    "    except:       \n",
    "        print (\"    'outc_reported_time' creation failed...         #WARNING: failure on load in \" + project_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Define function that creates the dataframe \"Outcome_Progress_Top\" through additions to \"outc_reported_time\":\n",
    "\n",
    "def outc_time_top(DIM_outcome_progress, outc_reported_time):\n",
    "    try:\n",
    "        outcome_progress_top = outc_reported_time[[\"Indicator ID\",\"Real Progress\"]]\n",
    "\n",
    "        outcome_progress_top=outcome_progress_top.groupby(by=\"Indicator ID\",dropna=True).sum()\n",
    "\n",
    "        outcome_progress_top.rename(columns = {'Real Progress':'Sum_of_Progress'}, inplace=True)\n",
    "\n",
    "        outcome_progress_top= outcome_progress_top.merge(DIM_outcome_progress, on=\"Indicator ID\")\n",
    "\n",
    "        outcome_progress_top=outcome_progress_top[[\"indicator_key\",\"Project Code\", \"Indicator ID\", \"Indicator Total Weight in LogFrame\", \"Sum_of_Progress\", \"Version\"]]\n",
    "\n",
    "        outcome_progress_top_indexed=outcome_progress_top.set_index(\"indicator_key\", inplace=True)\n",
    "        csv_name=project_code+\"_outc_progress_top.csv\"\n",
    "        outcome_progress_top.to_csv(file_destination+csv_name)\n",
    "        upload_file_to_sharepoint(file_destination,csv_name)\n",
    "        \n",
    "        \n",
    "        print (\"    'outcome_progress_top' created...\")\n",
    "\n",
    "        return outcome_progress_top\n",
    "\n",
    "    except:       \n",
    "        print (\"    'outcome_progress_top' creation failed...       #WARNING: failure on load in \" + project_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Define function that creates the DIM dataframe for activity indicators of type \"progress\":\n",
    "\n",
    "def dim_act_progress (Activities, project_code, timestamp):\n",
    "\n",
    "    try:\n",
    "        DIM_activities_progress = Activities\n",
    "\n",
    "        DIM_activities_progress [\"Indicator level\"]=\"Activities\"\n",
    "        DIM_activities_progress [\"Project Code\"]=project_code\n",
    "        DIM_activities_progress [\"Version\"]= timestamp\n",
    "        DIM_activities_progress [\"indicator_key\"]=DIM_activities_progress[\"Project Code\"]+\"_\"+DIM_activities_progress[\"Indicator ID\"]\n",
    "\n",
    "        #Filter \"progress\" rows and select columns\n",
    "        DIM_activities_progress = Activities[Activities[\"Indicator Type\"]==\"Progress\"][[\"indicator_key\",\"Project Code\",\"Indicator level\", \n",
    "                                            \"Activity ID\", \"Activities (Output) (A)\", \"Indicator ID\",\"Definition of the indicator\",\n",
    "                                            \"Overall target\", \"Indicator Total Weight in LogFrame\",\n",
    "                                            \"Specification of the indicator (if needed after reading column D)\",\"Source of verification\", \n",
    "                                            \"Predicted number of events within this indicator\", \"Indicator Type\",\"Version\"]]\n",
    "\n",
    "        DIM_activities_progress_indexed=DIM_activities_progress.set_index(\"indicator_key\")\n",
    "        csv_name=project_code+\"_DIM_activities_progress.csv\"\n",
    "        DIM_activities_progress_indexed.to_csv(file_destination+csv_name)\n",
    "        upload_file_to_sharepoint(file_destination,csv_name)\n",
    "       \n",
    "    \n",
    "        print (\"\")\n",
    "        print (\"Setting activities dataframes for \" + project_code)\n",
    "        print (\"\")\n",
    "        print (\"    'DIM_activities_progress' created...\")\n",
    "\n",
    "        return DIM_activities_progress\n",
    "    \n",
    "    except:\n",
    "        print (\"    'DIM_activities_progress' creation failed...         #WARNING: failure on load in \" + project_code)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Define function that creates the dataframe \"act_expected_time\" \n",
    "#by merging DIM Activities and Indicators, filtering, and implementing some adjustments:\n",
    "\n",
    "def act_exp_time(DIM_activities_progress, Indicators):\n",
    "    try:\n",
    "        act_expected_time=DIM_activities_progress.merge(Indicators, on=\"Indicator ID\")\n",
    "        act_expected_time[\"event_key\"]=act_expected_time[\"Project Code\"]+\"_\" +act_expected_time[\"Event ID\"]\n",
    "\n",
    "        act_expected_time= act_expected_time[[\"event_key\",\"Project Code\",\"Activity_ID/Outcome_ID\",\"Indicator ID\",\"Description\",\"Event ID\",\n",
    "                                                    \"Planned date of the event (what is expected)\",\n",
    "                                                    \"Planned value of the indicator (what is expected)\", \"Indicator level\", \n",
    "                                                    \"Overall target\",\"Indicator Total Weight in LogFrame\",\"Indicator Type\",\"Version\"]]\n",
    "\n",
    "        act_expected_time=act_expected_time[(act_expected_time[\"Indicator level\"]==\"Activities\") & (act_expected_time[\"Indicator Type\"]==\"Progress\")]\n",
    "\n",
    "        #Creates calculated column \"Expected_progress\"\n",
    "        act_expected_time[\"Expected_progress\"]=act_expected_time[\"Planned value of the indicator (what is expected)\"]/act_expected_time[\"Overall target\"]\n",
    "        #Creates calculated column \"Expected_Progress_Weighted\"\n",
    "        act_expected_time[\"Expected_Progress_Weighted\"]=act_expected_time[\"Expected_progress\"]*act_expected_time[\"Indicator Total Weight in LogFrame\"]\n",
    "\n",
    "        print (\"    'act_expected_time' ckecks have started...\")\n",
    "        \n",
    "        #Format universal time into \"dd/mm/yyyy\" and ammend invaid dates if necessary:\n",
    "        act_expected_time=debug_date_column(act_expected_time) #function n.15\n",
    "        #Check for data consistency: indicator´s overal target in M+E Plan must match the sum of planned values of all progress events:\n",
    "        target_vs_values(DIM_activities_progress, act_expected_time) #function n.16\n",
    "        \n",
    "        \n",
    "        act_expected_time_indexed=act_expected_time.set_index(\"event_key\")\n",
    "        csv_name=project_code+\"_act_expected_time.csv\"\n",
    "        act_expected_time_indexed.to_csv(file_destination+csv_name)\n",
    "        upload_file_to_sharepoint(file_destination,csv_name)\n",
    "\n",
    "        \n",
    "        \n",
    "        print (\"    'act_expected_time' created...\")\n",
    "\n",
    "\n",
    "        return act_expected_time\n",
    "   \n",
    "    except:       \n",
    "        print (\"    'act_expected_time' creation failed...         #WARNING: failure on load in \" + project_code)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Define function that creates the dataframe \"act_reported_time\" \n",
    "#by merging DIM Activities and Indicators, filtering, and implementing some adjustments:\n",
    "def act_rep_time(DIM_activities_progress, Indicators):\n",
    "\n",
    "    try:\n",
    "        act_reported_time=DIM_activities_progress.merge(Indicators, on=\"Indicator ID\")\n",
    "        act_reported_time[\"event_key\"]=act_reported_time[\"Project Code\"]+\"_\" +act_reported_time[\"Event ID\"]\n",
    "\n",
    "        act_reported_time= act_reported_time[[\"event_key\",\"Project Code\", \"Activity_ID/Outcome_ID\",\"Indicator ID\",\"Description\",\"Event ID\",\n",
    "                                                    \"Actual date of the event (when it really happened)\",\n",
    "                                                    \"Value of the indicator (real value after event)\", \"Indicator level\", \n",
    "                                                    \"Overall target\",\"Indicator Total Weight in LogFrame\",\"Indicator Type\", \"Version\"]]\n",
    "\n",
    "        act_reported_time=act_reported_time[(act_reported_time[\"Indicator level\"]==\"Activities\") & (act_reported_time[\"Indicator Type\"]==\"Progress\")]\n",
    "\n",
    "        #Drop non-reported values:\n",
    "        act_reported_time=act_reported_time.dropna()\n",
    "\n",
    "        #Create calculated column \"Real_progress\"\n",
    "        act_reported_time[\"Real_progress\"]=act_reported_time[\"Value of the indicator (real value after event)\"]/act_reported_time[\"Overall target\"]\n",
    "\n",
    "        print (\"    'act_reported_time' ckecks have started...\")\n",
    "\n",
    "        #Format universal time into \"dd/mm/yyyy\" and introduce corrections if necessary:\n",
    "        act_reported_time=debug_date_column(act_reported_time) #function n.15\n",
    "        \n",
    "        act_reported_time_indexed=act_reported_time.set_index(\"event_key\")\n",
    "        csv_name=project_code+\"_act_reported_time.csv\"\n",
    "        act_reported_time_indexed.to_csv(file_destination+csv_name)\n",
    "        upload_file_to_sharepoint(file_destination,csv_name)\n",
    "\n",
    "        \n",
    "        \n",
    "        print (\"    'act_reported_time' created...\")\n",
    "\n",
    "        return act_reported_time\n",
    "    \n",
    "    except:\n",
    "        print (\"    'act_reported_time' creation failed...         #WARNING: failure on load in \" + project_code)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. Define a function that creates the dataframe \"Activity_Progress_Top\" through additions to \"act_reported_time\":\n",
    "\n",
    "def act_time_top(DIM_activities_progress, act_reported_time):\n",
    "\n",
    "    try:\n",
    "\n",
    "        activity_progress_top = act_reported_time[[\"Indicator ID\",\"Real_progress\"]]\n",
    "\n",
    "        activity_progress_top=activity_progress_top.groupby(by=\"Indicator ID\").sum()\n",
    "\n",
    "        activity_progress_top.rename(columns = {'Real_progress':'Sum_of_Progress'}, inplace=True)\n",
    "\n",
    "        activity_progress_top= activity_progress_top.merge(DIM_activities_progress, on=\"Indicator ID\")\n",
    "\n",
    "        activity_progress_top=activity_progress_top[[\"indicator_key\",\"Project Code\", \"Indicator ID\", \"Indicator Total Weight in LogFrame\", \"Sum_of_Progress\", \"Version\"]]\n",
    "\n",
    "        activity_progress_top_indexed=activity_progress_top.set_index(\"Indicator ID\", inplace=True)\n",
    "        \n",
    "        csv_name=project_code+\"_activity_progress_top.csv\"\n",
    "        \n",
    "        activity_progress_top.to_csv(file_destination+csv_name)\n",
    "        upload_file_to_sharepoint(file_destination,csv_name)\n",
    "\n",
    "        \n",
    "        print (\"    'activity_progress_top' created...\")\n",
    "\n",
    "        return activity_progress_top\n",
    "\n",
    "    except:       \n",
    "        print (\"    'activity_progress_top' creation failed...     #WARNING: failure on load in \" + project_code)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13. Define a function that creates the dataframe \"Milestones\":\n",
    "def milestones_df(Milestones,project_code, timestamp):\n",
    "    try:\n",
    "        milestones=Milestones\n",
    "        milestones.rename(columns = {'Progress expected':'Progress expected (%)'}, inplace=True)\n",
    "        milestones[\"Project Code\"]=project_code\n",
    "        milestones[\"Version\"]=timestamp\n",
    "        milestones[\"milestone_key\"]=milestones[\"Project Code\"]+milestones[\"Event ID\"]\n",
    "        milestones=milestones[[\"milestone_key\",\"Project Code\",\"Indicator ID\",\"Event ID\",\"Description of the milestone\",\"Progress expected (%)\",\"Version\"]]\n",
    "\n",
    "        milestones_indexed=milestones.set_index(\"milestone_key\")\n",
    "        csv_name=project_code+\"_milestones.csv\"\n",
    "        milestones_indexed.to_csv(file_destination+csv_name)\n",
    "        upload_file_to_sharepoint(file_destination,csv_name)\n",
    "\n",
    "\n",
    "        print (\"\")\n",
    "        print (\"Setting milestones dataframe for \" + project_code)\n",
    "        print (\"\")\n",
    "        print (\"    'milestones' created...\")\n",
    "\n",
    "        return milestones\n",
    "\n",
    "    except:       \n",
    "        print (\"    'milestones' creation failed...         #WARNING: failure on load\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14. Steps to upload the resulting CSV files to SharePoint:\n",
    "def upload_file_to_sharepoint(source_folder,file_name):\n",
    "      \n",
    "    #a. Reads the file in OneDrive:\n",
    "    path=source_folder+file_name\n",
    "    with open(path, 'rb') as content_file:\n",
    "        file_content = content_file.read()\n",
    "    \n",
    "    #b. Connecting to the desired folder in the tennant:\n",
    "    target_url=\"/sites/Group-HPRS/Sdilene%20dokumenty/Data_loads-Do_not_modify/individual_inditracks\"\n",
    "    target_folder = ctx2.web.get_folder_by_server_relative_url(target_url)\n",
    "\n",
    "    #c. Upload the file to SharePoint\n",
    "    name = os.path.basename(path)\n",
    "    target_file = target_folder.upload_file(name, file_content).execute_query()    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15. Correct the invalid dates in a Pandas Series from the original dataset into valid dates in an output dataset:\n",
    "def debug_date_column(reviewed_dataset):     \n",
    "    reviewed_series = reviewed_dataset.iloc[:,6]\n",
    "    try: \n",
    "        reviewed_series=reviewed_series.dt.strftime('%d/%m/%Y')\n",
    "    \n",
    "    except:\n",
    "        reviewed_dataset[\"Corrected dates\"]=''\n",
    "        \n",
    "        for index, value in reviewed_series.iteritems():\n",
    "            if isinstance (value, datetime):\n",
    "                value_to_str=value.strftime('%d/%m/%Y')\n",
    "                reviewed_dataset[\"Corrected dates\"][index] = value_to_str\n",
    "                \n",
    "            else: \n",
    "                value=value.replace(\"/\",\".\")\n",
    "                day_wrong_date = value.split(\".\")[0]\n",
    "                month_wrong_date = value.split(\".\")[1]\n",
    "                year_wrong_date = value.split(\".\")[2]\n",
    "                if month_wrong_date == \"02\":\n",
    "                    day_fixed_date = \"28\"\n",
    "                elif day_wrong_date == \"31\":\n",
    "                    day_fixed_date = \"30\"\n",
    "                else: \n",
    "                    day_fixed_date = \"01\"\n",
    "                \n",
    "                fixed_value = datetime(int(year_wrong_date), int(month_wrong_date), int(day_fixed_date))\n",
    "                \n",
    "                value_to_str=fixed_value.strftime('%d/%m/%Y')\n",
    "                \n",
    "                reviewed_dataset[\"Corrected dates\"][index] = value_to_str\n",
    "   \n",
    "                print(\"                                                   Warning in event \" + reviewed_dataset[\"Event ID\"][index]+ \": event contains invalid dates\")\n",
    "\n",
    "\n",
    "        reviewed_dataset.iloc[:,6]=reviewed_dataset[\"Corrected dates\"]\n",
    "        del reviewed_dataset['Corrected dates']\n",
    "\n",
    "    return reviewed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16. Check if target in \"M+E Plan\" and the sum of all values in \"Events\" sum up the same in all progress indicators.\n",
    "    #In case it does not, it checks if all the Indicators of the project are present in the \"Events\" tab.\n",
    "    #Finally, it search for the indicators that are causing the mismatch between the dimension and the facts tables:\n",
    "def target_vs_values(DIM_table, events_table):\n",
    "    \n",
    "    sum_of_targets = DIM_table[\"Overall target\"].sum()\n",
    "    sum_of_events = events_table.loc[:,\"Planned value of the indicator (what is expected)\"].sum()\n",
    "    if sum_of_targets != sum_of_events:\n",
    "        #check if all indicators IDs are presents in both tables:\n",
    "        list_of_all_ids = DIM_table[\"Indicator ID\"].tolist()\n",
    "        list_of_ids_present_in_events = events_table[\"Indicator ID\"].tolist()\n",
    "        list_of_present_ids = [ind_id for ind_id in list_of_all_ids if ind_id in list_of_ids_present_in_events] #list comprehension compiles the valid indicator IDs set up in the \"M+E Plan\"\n",
    "        list_of_missing_ids = ', '.join(map(str,[ind_id for ind_id in list_of_all_ids if ind_id not in list_of_ids_present_in_events]))\n",
    "        print(\"                                                   #Warning: the following indicators in M+E Plan\")\n",
    "        print(\"                                                             are not present in the Events tab:\")\n",
    "        print(\"                                                             \"+ list_of_missing_ids)\n",
    "        \n",
    "        #group the values in the events table by indicator and find out what specific indicators are causing the mismatch with the \"M+E Plan\":\n",
    "        DIM_table_filtered=DIM_table[DIM_table[\"Indicator ID\"].isin(list_of_present_ids)]\n",
    "        event_values_df = events_table.loc[:,[\"Indicator ID\",\"Planned value of the indicator (what is expected)\"]].groupby(by=\"Indicator ID\").sum()\n",
    "        value_column_name = event_values_df.columns[0]\n",
    "        for index, row in DIM_table_filtered.iterrows():\n",
    "            indicator_id = row[\"Indicator ID\"]\n",
    "            indicator_value = event_values_df.loc[indicator_id,value_column_name]\n",
    "            target_value = row[\"Overall target\"]\n",
    "\n",
    "            if indicator_value != target_value:\n",
    "                print(\"                                                   #Warning in Indicator \" + indicator_id + \": Target does not\")\n",
    "                print(\"                                                            match the sum of events values in the column\")\n",
    "                print(\"                                                            '\" + value_column_name + \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17. Run program and create individual CSVs for each project:\n",
    "print(current_inditracks[['Country', 'Project_code']])\n",
    "ctx2=autenticate_in_sharepoint(url_hprs,USERNAME,PASSWORD) #function n.3, for uploading docs to HPRS SharePoint\n",
    "\n",
    "for index, row in df_shp_sites.iterrows():\n",
    "    shp_country = row['country']\n",
    "    url = row['site']\n",
    "    try:\n",
    "        ctx=autenticate_in_sharepoint(url,USERNAME,PASSWORD) #function n.3, for dowloading tables from each Country SharePoint\n",
    "\n",
    "        for index, row in current_inditracks.iterrows():\n",
    "            project_code= row['Project_code']\n",
    "            inditrack_country=row['Country']\n",
    "            relative_url=row['IndiTrack_Relative_url']\n",
    "\n",
    "            if shp_country ==inditrack_country:\n",
    "\n",
    "                try:\n",
    "\n",
    "                    Outcomes,Activities, Indicators, Milestones = download_inditrack(ctx, project_code, relative_url) #function n.4\n",
    "\n",
    "                    #14a. Create csv for DIM_outcome_progress: \n",
    "                    DIM_outcome_progress =dim_outc_progress(Outcomes, project_code, timestamp) #function n.5\n",
    "\n",
    "                    #14b. Create csv for outc_expected_time:\n",
    "                    outc_expected_time = outc_exp_time(DIM_outcome_progress, Indicators) #function n.6\n",
    "\n",
    "                    #14c. Create csv for outc_reported_time:\n",
    "                    outc_reported_time = outc_rep_time(DIM_outcome_progress, Indicators) #function n.7\n",
    "\n",
    "                    #14d. Create csv for outcome_progress_top:\n",
    "                    outcome_progress_top = outc_time_top(DIM_outcome_progress, outc_reported_time) #function n.8\n",
    "\n",
    "                    #14e. Create csv for DIM_activities_progress:\n",
    "                    DIM_activities_progress = dim_act_progress (Activities, project_code, timestamp) #function n.9\n",
    "\n",
    "                    #14f. Create csv for act_expected_time:\n",
    "                    act_expected_time = act_exp_time(DIM_activities_progress, Indicators) #function n.10\n",
    "\n",
    "                    #14g. Create csv for act_reported_time:\n",
    "                    act_reported_time = act_rep_time(DIM_activities_progress, Indicators) #function n.11\n",
    "\n",
    "                    #14h. Create csv for activity_progress_top:\n",
    "                    activity_progress_top = act_time_top(DIM_activities_progress, act_reported_time) #function n.12\n",
    "\n",
    "                    #14i. Create csv for milestones:\n",
    "                    milestones= milestones_df(Milestones,project_code, timestamp) #function n.13\n",
    "\n",
    "                    print (\"\")\n",
    "                    print('Iteration for ' + project_code + ' has been sucessful')        \n",
    "\n",
    "                except:       \n",
    "                    print (\"\")\n",
    "                    print('Iteration for ' + project_code + ' has been cancelled')\n",
    "\n",
    "                    \n",
    "    except:\n",
    "        print(\"The Automat has failed to connect to \" + shp_country + \"'s SharePoint site\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
